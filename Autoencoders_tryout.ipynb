{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoders.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxj1V3xLm0eG"
      },
      "source": [
        "#Testing out VAE on basic shapes | Circles, rectangles, triangles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "radvl2Obm2tb"
      },
      "source": [
        "Autoencoder, generation of shapes, and information rate\n",
        "\n",
        "a. Create a database of sufficient size of 28*28 images with circles, triangles and rectangles at random locations.\n",
        "\n",
        "b. Create a suitable variational autoencoder (VAE) for the data in the database. Motivate the network configuration  (architecture, objective functions, hyperparameters) you select. \n",
        "\n",
        "c. Use your VAE to generate new dataand provide a quantitative measure of the performance of your system\n",
        "\n",
        "d. Replace your VAE with a similar but not identical system that does not use the ELBO. Instead, control the distribution of the latent layer to be iid Gaussian (eg with MMD) and add Gaussian noise with a fixed that variance you set\n",
        "\n",
        "e. Recalling that mutual information is given by $I(X;Y) = E[log\\frac{p(X,Y)}{p(X)p(Y)}]$, estimate the information passing through the latent layer, in bits. Explain what this information represents in terms of your reconstruction. \n",
        "\n",
        "f. Compare the performance of the systems, using suitable attributes and/or extensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnGb4QW-ksDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ba5211-01c4-40a8-b26d-a9be8c8a1113"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
        "from keras.utils import np_utils\n",
        "#from keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prt4xZ6BuwcQ"
      },
      "source": [
        "### Shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq0Z6Hfouxra"
      },
      "source": [
        "# config values\n",
        "NUM_OF_DATA = 1000\n",
        "\n",
        "WIDTH = 28\n",
        "HEIGHT = 28  #The above 3 will be used throughout\n",
        "\n",
        "WHITE = (255,255,255)\n",
        "BLACK = (0,0,0)\n",
        "\n",
        "MIN_THICKNESS = 1\n",
        "MAX_THICKNESS = 3\n",
        "###\n",
        "\n",
        "CENTER_X = WIDTH //2\n",
        "CENTER_Y = HEIGHT//2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwlx_Pu5uz0d"
      },
      "source": [
        "tri = []\n",
        "for _ in range(NUM_OF_DATA):\n",
        "    img = np.zeros((WIDTH, HEIGHT, 3), np.uint8) + 255\n",
        "\n",
        "#triangles\n",
        "    t_height = random.randint(HEIGHT//5, 4*HEIGHT//5)\n",
        "    t_width  = random.randint(WIDTH //5, 4*WIDTH //5)\n",
        "    t_dir    = random.randint(0,1) * 2 - 1\n",
        "    #Random center\n",
        "    x_bias = random.randint(t_width - WIDTH, WIDTH - t_width)//2\n",
        "    y_bias = random.randint(t_height - HEIGHT, HEIGHT - t_height)//2\n",
        "\n",
        "    t_thickness = random.randint(MIN_THICKNESS, MAX_THICKNESS)\n",
        "\n",
        "    vertices = np.array([\n",
        "        [CENTER_X + x_bias, CENTER_Y + t_height//2 * t_dir + y_bias],\n",
        "        [CENTER_X - t_width//2 + x_bias, CENTER_Y - t_height//2 * t_dir + y_bias],\n",
        "        [CENTER_X + t_width//2 + x_bias, CENTER_Y - t_height//2 * t_dir + y_bias]\n",
        "    ])\n",
        "    pts = vertices.reshape((-1, 1, 2))\n",
        "    cv2.polylines(img, [pts], isClosed = True, color = BLACK, thickness = t_thickness)\n",
        "\n",
        "    tri.append([img, (1,0,0)]) #label to indicate shape\n",
        "\n",
        "temptri = img_to_array(tri[0][0])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RuhWaLqu6VW"
      },
      "source": [
        "rect = []\n",
        "for _ in range(NUM_OF_DATA):\n",
        "    img = np.zeros((WIDTH, HEIGHT, 3), np.uint8) + 255\n",
        "\n",
        "#rectangles\n",
        "    r_height = random.randint(HEIGHT//5, 4*HEIGHT//5)\n",
        "    r_width  = random.randint(WIDTH //5, 4*WIDTH //5)\n",
        "\n",
        "    #Random center\n",
        "    x_bias = random.randint(r_width - WIDTH, WIDTH - r_width)//2\n",
        "    y_bias = random.randint(r_height - HEIGHT, HEIGHT - r_height)//2\n",
        "\n",
        "    r_thickness = random.randint(MIN_THICKNESS, MAX_THICKNESS)\n",
        "\n",
        "    vertices = np.array([\n",
        "        [CENTER_X + r_width//2 + x_bias, CENTER_Y + r_height//2 + y_bias],\n",
        "        [CENTER_X - r_width//2 + x_bias, CENTER_Y + r_height//2 + y_bias],\n",
        "        [CENTER_X - r_width//2 + x_bias, CENTER_Y - r_height//2 + y_bias],\n",
        "        [CENTER_X + r_width//2 + x_bias, CENTER_Y - r_height//2 + y_bias]\n",
        "    ])\n",
        "    pts = vertices.reshape((-1, 1, 2))\n",
        "    cv2.polylines(img, [pts], isClosed = True, color = BLACK, thickness = r_thickness)\n",
        "\n",
        "    rect.append([img, (0,1,0)])\n",
        "\n",
        "temprectangle = img_to_array(rect[0][0])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFtwAtUhu-MR"
      },
      "source": [
        "circ = []\n",
        "for _ in range(NUM_OF_DATA):\n",
        "    img = np.zeros((WIDTH, HEIGHT, 3), np.uint8) + 255\n",
        "\n",
        "#circles\n",
        "    c_rad = random.randint(max(WIDTH, HEIGHT) // 5, 2 * max(WIDTH, HEIGHT) // 5)\n",
        "\n",
        "    c_thickness = random.randint(MIN_THICKNESS, MAX_THICKNESS)\n",
        "\n",
        "    x_bias = random.randint(c_rad * 2 - WIDTH, WIDTH - c_rad * 2)//2\n",
        "    y_bias = random.randint(c_rad * 2 - HEIGHT, HEIGHT - c_rad * 2)//2\n",
        "\n",
        "    cv2.circle(img,\n",
        "        center = (CENTER_X + x_bias, CENTER_Y + y_bias),\n",
        "        radius = c_rad,\n",
        "        color = BLACK,\n",
        "        thickness = r_thickness\n",
        "    )\n",
        "\n",
        "    circ.append([img, (0,0,1)])\n",
        "\n",
        "tempcircle = img_to_array(circ[0][0])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKeD1GURvCTM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "03469b86-9d0c-4305-e937-02684c63a824"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(tri[3][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9f515de9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C27PkCFs0IKu"
      },
      "source": [
        "## Once again, we join the datasets and shuffle them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cUWnWwG-WKb"
      },
      "source": [
        "Rerun the cell below to create a new dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5mxueOL0Hp7"
      },
      "source": [
        "#tot_data = [(triangle for triangle in tri ),(circle for circle in circ), ( rectangle for rectangle in rect)]\n",
        "\n",
        "#tot_data[0]\n",
        "# import random\n",
        "# random.shuffle(tot_data)\n",
        "# for s in tot_data[:10]:\n",
        "#   print(s)\n",
        "\n",
        "tot_data = []\n",
        "for t in tri: tot_data.append([cv2.cvtColor(t[0],cv2.COLOR_BGR2GRAY),t[1]])\n",
        "for r in rect: tot_data.append([cv2.cvtColor(r[0],cv2.COLOR_BGR2GRAY),r[1]])\n",
        "for c in circ: tot_data.append([cv2.cvtColor(c[0],cv2.COLOR_BGR2GRAY),c[1]])\n",
        "random.shuffle(tot_data)\n",
        "#tot_data[0]\n",
        "\n",
        "data_x = []\n",
        "data_y = []\n",
        "for d in tot_data:\n",
        "    data_x.append(d[0])\n",
        "    data_y.append(d[1])\n",
        "\n",
        "# xx = [data for data in tot_data[:][0]]\n",
        "# #print(xx)\n",
        "# #print(data_x[:10])\n",
        "# #print(data_x[0], data_y[0])\n",
        "# np.array(data_x).shape\n",
        "# data_y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq6ZqucY0MkF"
      },
      "source": [
        "number_of_classes = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSpEXAjd8ARp"
      },
      "source": [
        "TRAIN = 1200\n",
        "TEST = NUM_OF_DATA*3 - TRAIN\n",
        "x = np.array(data_x[:TRAIN]).reshape(TRAIN, 28, 28, 1)\n",
        "y = np.array(data_y[:TRAIN])+0.\n",
        "xt = np.array(data_x[TRAIN:]).reshape(TEST, 28, 28, 1)\n",
        "yt = np.array(data_y[TRAIN:])+0.\n",
        "print(x.shape, y.shape)\n",
        "print(xt.shape, yt.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5tTa41M8NPb"
      },
      "source": [
        "import datetime\n",
        "# rm -rf ./logs/\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)) #x and y are training datasets to be defined beforehand\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((xt, yt)) #xt and yt are test datasets to be defined beforehand\n",
        "\n",
        "print(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-tESB1e9dy9"
      },
      "source": [
        "Let us take a number of images from the dataset of images we have (essentially without the groundstruth labels), and run them through a VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "YTBEemOJ8N3o",
        "outputId": "b0f1471c-bc6f-444f-82f3-8aa4866bb39a"
      },
      "source": [
        "plt.imshow(data_x[2])   #Eg of one of our images"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9f4cf54510>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAALI0lEQVR4nO3dTYhd9RnH8d+vY1xULeStwzSGxkq6CKWNZQgFpVikGrOJbsQsJAVhXCgouKjYhS5DqUoXRRhrMC1WEVTMIjSmQQhurFdJ82JaYyVipmMmJgsjhWrGp4s5kWuc++I9595zMs/3A8PcOfdOzsNNvp4753/H44gQgKXvW3UPAGA0iB1IgtiBJIgdSILYgSQuG+XOVq0Yi3Vrl41yl0AqJz78XB+fnfdi95WK3fZmSb+XNCbpjxGxo9vj161dpr/vXVtmlwC62HTLhx3vG/hlvO0xSX+QdKukDZK22d4w6J8HYLjK/My+SdJ7EfF+RHwm6XlJW6sZC0DVysS+RlL7a4aTxbavsD1lu2W7dfrMfIndAShj6GfjI2I6IiYjYnL1yrFh7w5AB2Vin5HUfrbt6mIbgAYqE/ubktbbvsb25ZLulLS7mrEAVG3gpbeIOG/7Pkl7tbD0tjMijlY2GYBKlVpnj4g9kvZUNAuAIeLtskASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASI/1fSfdyy/c21j0CIEna+5+DdY9QOY7sQBLEDiRB7EASxA4kQexAEsQOJEHsQBIjXWd/99C3WUvHJaHXv9NLcR2eIzuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSZR6U43tE5LOSZqXdD4iJqsYCkD1qngH3S8i4uMK/hwAQ8TLeCCJsrGHpFdtv2V7arEH2J6y3bLd+lz/K7k7AIMq+zL+hoiYsf1dSfts/zMiDrQ/ICKmJU1L0ne8IkruD8CASh3ZI2Km+Dwn6WVJm6oYCkD1Bo7d9hW2r7pwW9LNko5UNRiAapV5GT8u6WXbF/6cv0TEX7t9ww9//F/t3dv594Cb/Lvul+LvLzcdf9+jNXDsEfG+pJ9UOAuAIWLpDUiC2IEkiB1IgtiBJIgdSKJRl2zutdxR51JNt30vxWWaqjR1eS3j3xlHdiAJYgeSIHYgCWIHkiB2IAliB5IgdiCJRq2z99JtbbSpa/DS0l7Tbeo6urS0n/dBcGQHkiB2IAliB5IgdiAJYgeSIHYgCWIHkrik1tm7uVR/F15q9now6+hLB0d2IAliB5IgdiAJYgeSIHYgCWIHkiB2IIkls87eC+vwg+27TqyjV6vnkd32Tttzto+0bVthe5/t48Xn5cMdE0BZ/byMf0bS5ou2PSRpf0Ssl7S/+BpAg/WMPSIOSDp70eatknYVt3dJuq3iuQBUbNATdOMRMVvc/kjSeKcH2p6y3bLdOn1mfsDdASir9Nn4iAhJ0eX+6YiYjIjJ1SvHyu4OwIAGjf2U7QlJKj7PVTcSgGEYNPbdkrYXt7dLeqWacQAMS891dtvPSbpR0irbJyU9ImmHpBds3y3pA0l3DHPIUViq6/Cso+OCnrFHxLYOd91U8SwAhoi3ywJJEDuQBLEDSRA7kASxA0mk+RXXspq8vFX3/rthea05OLIDSRA7kASxA0kQO5AEsQNJEDuQBLEDSbDOXoEm/3rssLGOfungyA4kQexAEsQOJEHsQBLEDiRB7EASxA4kwTr7CFzK6/Csoy8dHNmBJIgdSILYgSSIHUiC2IEkiB1IgtiBJFhnb4A61+FZR8+j55Hd9k7bc7aPtG171PaM7YPFx5bhjgmgrH5exj8jafMi25+IiI3Fx55qxwJQtZ6xR8QBSWdHMAuAISpzgu4+24eKl/nLOz3I9pTtlu3W6TPzJXYHoIxBY39S0rWSNkqalfRYpwdGxHRETEbE5OqVYwPuDkBZA8UeEaciYj4ivpD0lKRN1Y4FoGoDxW57ou3L2yUd6fRYAM3Qc53d9nOSbpS0yvZJSY9IutH2Rkkh6YSke4Y4Y3qshaMKPWOPiG2LbH56CLMAGCLeLgskQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASPWO3vdb2a7bfsX3U9v3F9hW299k+XnxePvxxAQyqnyP7eUkPRsQGST+TdK/tDZIekrQ/ItZL2l98DaChesYeEbMR8XZx+5ykY5LWSNoqaVfxsF2SbhvWkADK+0Y/s9teJ+k6SW9IGo+I2eKujySNd/ieKdst263TZ+ZLjAqgjL5jt32lpBclPRARn7TfFxEhKRb7voiYjojJiJhcvXKs1LAABtdX7LaXaSH0ZyPipWLzKdsTxf0TkuaGMyKAKvRzNt6SnpZ0LCIeb7trt6Ttxe3tkl6pfjwAVbmsj8dcL+kuSYdtHyy2PSxph6QXbN8t6QNJdwxnRABV6Bl7RLwuyR3uvqnacQAMC++gA5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkujn+uxrbb9m+x3bR23fX2x/1PaM7YPFx5bhjwtgUP1cn/28pAcj4m3bV0l6y/a+4r4nIuJ3wxsPQFX6uT77rKTZ4vY528ckrRn2YACq9Y1+Zre9TtJ1kt4oNt1n+5DtnbaXd/ieKdst263TZ+ZLDQtgcH3HbvtKSS9KeiAiPpH0pKRrJW3UwpH/scW+LyKmI2IyIiZXrxyrYGQAg+grdtvLtBD6sxHxkiRFxKmImI+ILyQ9JWnT8MYEUFY/Z+Mt6WlJxyLi8bbtE20Pu13SkerHA1CVfs7GXy/pLkmHbR8stj0saZvtjZJC0glJ9wxlQgCV6Ods/OuSvMhde6ofB8Cw8A46IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5JwRIxuZ/ZpSR+0bVol6eORDfDNNHW2ps4lMdugqpzt+xGxerE7Rhr713ZutyJisrYBumjqbE2dS2K2QY1qNl7GA0kQO5BE3bFP17z/bpo6W1PnkphtUCOZrdaf2QGMTt1HdgAjQuxAErXEbnuz7X/Zfs/2Q3XM0IntE7YPF5ehbtU8y07bc7aPtG1bYXuf7ePF50WvsVfTbI24jHeXy4zX+tzVffnzkf/MbntM0ruSfinppKQ3JW2LiHdGOkgHtk9ImoyI2t+AYfvnkj6V9KeI+FGx7beSzkbEjuI/lMsj4tcNme1RSZ/WfRnv4mpFE+2XGZd0m6Rfqcbnrstcd2gEz1sdR/ZNkt6LiPcj4jNJz0vaWsMcjRcRBySdvWjzVkm7itu7tPCPZeQ6zNYIETEbEW8Xt89JunCZ8Vqfuy5zjUQdsa+R9GHb1yfVrOu9h6RXbb9le6ruYRYxHhGzxe2PJI3XOcwiel7Ge5Quusx4Y567QS5/XhYn6L7uhoj4qaRbJd1bvFxtpFj4GaxJa6d9XcZ7VBa5zPiX6nzuBr38eVl1xD4jaW3b11cX2xohImaKz3OSXlbzLkV96sIVdIvPczXP86UmXcZ7scuMqwHPXZ2XP68j9jclrbd9je3LJd0paXcNc3yN7SuKEyeyfYWkm9W8S1HvlrS9uL1d0is1zvIVTbmMd6fLjKvm5672y59HxMg/JG3Rwhn5f0v6TR0zdJjrB5L+UXwcrXs2Sc9p4WXd51o4t3G3pJWS9ks6LulvklY0aLY/Szos6ZAWwpqoabYbtPAS/ZCkg8XHlrqfuy5zjeR54+2yQBKcoAOSIHYgCWIHkiB2IAliB5IgdiAJYgeS+D9xu6AIfvYU+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWR5rDIMHW9q"
      },
      "source": [
        "### Sample Code 2 | but with MNIST dataset | Sourced from bnsreenu on github in \"python_for_microscopists/blob/master/178_179_variational_autoencoders_mnist\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZoU09-i8g1g"
      },
      "source": [
        "An autoencoder takes input data (x), reduces its dimensionality in the encoder segment, determines its distinguishing features inside the bottleneck (the dimensions of which we have initially set to 2 below), and attempts to recreate the input ( or a denoised input) through the decoder (essentially expanding it back to its original dimensions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7zfF6fcIWpD"
      },
      "source": [
        "Below is just how this would be done if we were using MNIST data, which we are not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9YbHZcY-Ew-"
      },
      "source": [
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#Normalize and reshape ============\n",
        "\n",
        "#Norm.\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pubOCFx3-Fmc",
        "outputId": "f821d28e-40b6-488a-bf91-4f86ebb6d50e"
      },
      "source": [
        "np.max(x_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLnFSDUQ-Jna",
        "outputId": "c6510b50-76ef-4cd6-b2e8-95657e2e8ed0"
      },
      "source": [
        "np.max(data_x[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W65FWJFIVtp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "9a214840-0907-41eb-fa36-be45a5ea44ce"
      },
      "source": [
        "\n",
        "\n",
        "# Reshape \n",
        "img_width  = x_train.shape[1]\n",
        "img_height = x_train.shape[2]\n",
        "num_channels = 1 #MNIST --> grey scale so 1 channel\n",
        "x_train = x_train.reshape(x_train.shape[0], img_height, img_width, num_channels)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_height, img_width, num_channels)\n",
        "input_shape = (img_height, img_width, num_channels)\n",
        "# ========================\n",
        "#View a few images\n",
        "plt.figure(1)\n",
        "plt.subplot(221)\n",
        "plt.imshow(x_train[42][:,:,0])\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.imshow(x_train[420][:,:,0])\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.imshow(x_train[4200][:,:,0])\n",
        "\n",
        "plt.subplot(224)\n",
        "plt.imshow(x_train[42000][:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcV4pMsvs2KW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6176b02e-b95d-4255-83b4-c92a425209ad"
      },
      "source": [
        "latent_dim = 2 # Number of latent dim parameters\n",
        "\n",
        "input_img = Input(shape=input_shape, name='encoder_input')\n",
        "x = Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "conv_shape = K.int_shape(x) #Shape of conv to be provided to decoder\n",
        "#Flatten\n",
        "x = Flatten()(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "# Two outputs, for latent mean and log variance (std. dev.)\n",
        "#Use these to sample random variables in latent space to which inputs are mapped. \n",
        "z_mu = Dense(latent_dim, name='latent_mu')(x)   #Mean values of encoded input\n",
        "z_sigma = Dense(latent_dim, name='latent_sigma')(x)  #Std dev. (variance) of encoded input\n",
        "\n",
        "#Combine the z layers mu and sigma to a sampling layer and outsource the sampling with epsilon | Reparameterization trick\n",
        "def sample_z(args):\n",
        "  z_mu, z_sigma = args\n",
        "  eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
        "  return z_mu + K.exp(z_sigma / 2) * eps\n",
        "\n",
        "z = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([z_mu, z_sigma])\n",
        "\n",
        "# Define and summarize encoder model.\n",
        "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n",
        "print(encoder.summary())\n",
        "\n",
        "# ================= ###########\n",
        "# Decoder\n",
        "#\n",
        "# ================= #################\n",
        "\n",
        "# decoder takes the latent vector as input\n",
        "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
        "\n",
        "# Need to start with a shape that can be remapped to original image shape as\n",
        "#we want our final utput to be same shape original input.\n",
        "#So, add dense layer with dimensions that can be reshaped to desired output shape\n",
        "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
        "# reshape to the shape of last conv. layer in the encoder, so we can \n",
        "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "# upscale (conv2D transpose) back to original shape\n",
        "# use Conv2DTranspose to reverse the conv layers defined in the encoder\n",
        "x = Conv2DTranspose(32, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
        "#Can add more conv2DTranspose layers, if desired. \n",
        "#Using sigmoid activation\n",
        "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
        "\n",
        "# Define and summarize decoder model\n",
        "decoder = Model(decoder_input, x, name='decoder')\n",
        "decoder.summary()\n",
        "\n",
        "# apply the decoder to the latent sample \n",
        "z_decoded = decoder(z)\n",
        "\n",
        "\n",
        "# =========================\n",
        "#Define custom loss\n",
        "#VAE is trained using two loss functions reconstruction loss and KL divergence\n",
        "#Let us add a class to define a custom layer with loss\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "\n",
        "    def vae_loss(self, x, z_decoded):\n",
        "        x = K.flatten(x)\n",
        "        z_decoded = K.flatten(z_decoded)\n",
        "        \n",
        "        # Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
        "        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
        "        \n",
        "        # KL divergence\n",
        "        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n",
        "        return K.mean(recon_loss + kl_loss)\n",
        "\n",
        "    # add custom loss to the class\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        z_decoded = inputs[1]\n",
        "        loss = self.vae_loss(x, z_decoded)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        return x\n",
        "\n",
        "# apply the custom loss to the input images and the decoded latent distribution sample\n",
        "y = CustomLayer()([input_img, z_decoded])\n",
        "# y is basically the original image after encoding input img to mu, sigma, z\n",
        "# and decoding sampled z values.\n",
        "#This will be used as output for vae\n",
        "\n",
        "# =================\n",
        "# VAE \n",
        "# =================\n",
        "vae = Model(input_img, y, name='vae')\n",
        "\n",
        "# Compile VAE\n",
        "vae.compile(optimizer='adam', loss=None)\n",
        "vae.summary()\n",
        "\n",
        "# Train autoencoder\n",
        "vae.fit(x_train, None, epochs = 10, batch_size = 32, validation_split = 0.2)\n",
        "\n",
        "# =================\n",
        "# Visualize results\n",
        "# =================\n",
        "#Visualize inputs mapped to the Latent space\n",
        "#Remember that we have encoded inputs to latent space dimension = 2. \n",
        "#Extract z_mu --> first parameter in the result of encoder prediction representing mean\n",
        "\n",
        "mu, _, _ = encoder.predict(x_test)\n",
        "#Plot dim1 and dim2 for mu\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(mu[:, 0], mu[:, 1], c=y_test, cmap='brg')\n",
        "plt.xlabel('dim 1')\n",
        "plt.ylabel('dim 2')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualize images\n",
        "#Single decoded image with random input latent vector (of size 1x2)\n",
        "#Latent space range is about -5 to 5 so pick random values within this range\n",
        "#Try starting with -1, 1 and slowly go up to -1.5,1.5 and see how it morphs from \n",
        "#one image to the other.\n",
        "sample_vector = np.array([[1,-1]])\n",
        "decoded_example = decoder.predict(sample_vector)\n",
        "decoded_example_reshaped = decoded_example.reshape(img_width, img_height)\n",
        "plt.imshow(decoded_example_reshaped)\n",
        "\n",
        "#Let us automate this process by generating multiple images and plotting\n",
        "#Use decoder to generate images by tweaking latent variables from the latent space\n",
        "#Create a grid of defined size with zeros. \n",
        "#Take sample from some defined linear space. In this example range [-4, 4]\n",
        "#Feed it to the decoder and update zeros in the figure with output.\n",
        "\n",
        "\n",
        "n = 20  # generate 15x15 digits\n",
        "figure = np.zeros((img_width * n, img_height * n, num_channels))\n",
        "\n",
        "#Create a Grid of latent variables, to be provided as inputs to decoder.predict\n",
        "#Creating vectors within range -5 to 5 as that seems to be the range in latent space\n",
        "grid_x = np.linspace(-5, 5, n)\n",
        "grid_y = np.linspace(-5, 5, n)[::-1]\n",
        "\n",
        "# decoder for each square in the grid\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
        "        figure[i * img_width: (i + 1) * img_width,\n",
        "               j * img_height: (j + 1) * img_height] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "#Reshape for visualization\n",
        "fig_shape = np.shape(figure)\n",
        "figure = figure.reshape((fig_shape[0], fig_shape[1]))\n",
        "\n",
        "plt.imshow(figure, cmap='gnuplot2')\n",
        "plt.show()  "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 28, 28, 32)   320         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 14, 14, 64)   18496       conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 12544)        0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           401440      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "latent_mu (Dense)               (None, 2)            66          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "latent_sigma (Dense)            (None, 2)            66          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 2)            0           latent_mu[0][0]                  \n",
            "                                                                 latent_sigma[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 494,244\n",
            "Trainable params: 494,244\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 12544)             37632     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "decoder_output (Conv2DTransp (None, 28, 28, 1)         289       \n",
            "=================================================================\n",
            "Total params: 56,385\n",
            "Trainable params: 56,385\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a8e341a2b983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# apply the custom loss to the input images and the decoded latent distribution sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_decoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;31m# y is basically the original image after encoding input img to mu, sigma, z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# and decoding sampled z values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 977\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1115\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    886\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-22-a8e341a2b983>:81 call  *\n        loss = self.vae_loss(x, z_decoded)\n    <ipython-input-22-a8e341a2b983>:74 vae_loss  *\n        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:1399 r_binary_op_wrapper\n        y, x = maybe_promote_tensors(y, x)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:1338 maybe_promote_tensors\n        *[_maybe_get_dtype(x) for x in nest.flatten(tensors)])\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/numpy_ops/np_dtypes.py:116 _result_type\n        dtype = np.result_type(*arrays_and_dtypes)\n    <__array_function__ internals>:6 result_type\n        \n\n    TypeError: Cannot interpret '<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'latent_sigma')>' as a data type\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFmMwnPh82Ns"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}